{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "#import rpy2.robjects as robjects\n",
    "#from rpy2.robjects import r, pandas2ri\n",
    "#from rpy2.robjects.packages import importr\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from os.path import exists\n",
    "import pickle\n",
    "from scipy.stats import sem\n",
    "import statistics\n",
    "import scipy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from kneed import KneeLocator\n",
    "import math\n",
    "\n",
    "# Change matplotlib font to Arial\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "\n",
    "### 'data_sf' is human players in self finding studies\n",
    "### 'keep_close' is the proximity baseline\n",
    "### 'human_extended' is the perturbation study for humans\n",
    "\n",
    "colors = {\"human\": \"#000000\", \"data_sf\": \"#8a3a01\", \"human_extended\": \"#525151\", \"self_class\": \"#19c202\", \"self_class_include_perturbation\": \"#19c202\",\n",
    "          \"keep_close\": \"#0f7801\", \"self_classshort\": \"#19c202\",\n",
    "          \"random\": \"#9c2200\", \"a2c_training\": \"#89CFF0\",\n",
    "          \"trpo_training\": \"#0000FF\",\n",
    "          \"acer_training\": \"#7393B3\", \"acer_training2\": \"#0096FF\", \"acer_trainingeasy\": \"#9c2200\", \"ppo2_training\": \"#0096FF\", \"dqn_training\": \"#5D3FD3\", \"option_critic\": \"#23dedb\"}\n",
    "\n",
    "game_titles = {\n",
    "    \"shuffleKeys_game\": \"Switching Mappings Keys Game\",\n",
    "    \"shuffleKeys_game_final\": \"Switching Mappings Keys Game\",\n",
    "    \"contingency_game\": \"Contingency Game\",\n",
    "    \"contingency_game_final\": \"Contingency Game\",\n",
    "    \"contingency_game_0\": \"Contingency Game (Single Seed)\",\n",
    "    \"contingency_game_lrtest\": \"Contingency Game (Single Seed)\",\n",
    "    \"contingency_game_r0\": \"Contingency Game (Agent Placement is Constant Among Each Level)\",\n",
    "    \"contingency_game_diff_color\": \"Contingency Game (Real Agent is Blue)\",\n",
    "    \"contingency_game_shuffled\": \"Switching Mappings Game (Switched Once in Every 200 Levels)\",\n",
    "    \"contingency_game_shuffled_1\": \"Switching Mappings Game (Switched Every Level)\",\n",
    "    \"contingency_game_shuffled_100\": \"Switching Mappings Game (Switched Once in Every 100 Levels)\",\n",
    "    \"contingency_game_shuffled_200\": \"Switching Mappings Game (Switched Once in Every 200 Levels)\",\n",
    "    \"contingency_game_shuffled_final\": \"Switching Mappings Game (Switched Once in Every 200 Levels)\",\n",
    "    \"contingency_game_shuffled_1_final\": \"Switching Mappings Game (Switched Every Level)\",\n",
    "    \"contingency_game_shuffled_100_final\": \"Switching Mappings Game (Switched Once in Every 100 Levels)\",\n",
    "    \"contingency_game_shuffled_200_final\": \"Switching Mappings Game (Switched Once in Every 200 Levels)\",\n",
    "    \"logic_game\": \"Logic Game\",\n",
    "    \"logic_game_final\": \"Logic Game\",\n",
    "    \"logic_game_0\": \"Logic Game\",\n",
    "    \"logic_game_lrtest\": \"Logic Game\",\n",
    "    \"logic_include_perturbation_game\": \"Logic Game (Modified After 2000 Levels)\",\n",
    "    \"change_agent_game\": \"Switching Embodiments Game\",\n",
    "    \"change_agent_game_final\": \"Switching Embodiments Game\",\n",
    "    \"change_agent_game_lrtest\": \"Switching Embodiments Game (Single Seed)\"\n",
    "}\n",
    "\n",
    "label_dict = {'human': 'Human', 'data_sf': 'Human (Self-Finding)', 'human_extended': 'Human include_perturbation',\n",
    "              'self_class': 'Self Class', 'self_class_include_perturbation': 'Self Class', 'self_classshort': 'Self Class', 'dqn_training': 'DQN',\n",
    "              'random': 'Random', 'a2c_training': \"A2C\", 'trpo_training': 'TRPO', 'ppo2_training': 'PPO2',\n",
    "              'acer_training': 'ACER', 'acer_training2': 'ACER (New Version)', 'acer_trainingeasy': 'ACER (Easy Version)', \"option_critic\": \"OC\", \"keep_close\": \"Keeping Close\"}\n",
    "\n",
    "\n",
    "def find_between(s, first, last):\n",
    "    try:\n",
    "        start = s.index(first) + len(first)\n",
    "        end = s.index(last, start)\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_seed_num_and_iter(x):\n",
    "    if \"seed\" in x:\n",
    "        return int(find_between(x.split(\"/\")[4], \"seed\", \"-\")) * 1000000000 + int(x.split(\"/\")[-1][6:-5])\n",
    "    else:\n",
    "        if \"short\" in x:\n",
    "            return int(x.split(\"/\")[4].split(\"iter\")[1]) * 1000000000 + int(x.split(\"/\")[-1][11:-5])\n",
    "        else:\n",
    "            return int(x.split(\"/\")[4].split(\"iter\")[1]) * 1000000000 + int(x.split(\"/\")[-1][6:-5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def cosmetic_change(game_type, ax, axes, last_100, agent_types, only_first_100, perturbated, avg_per_n_steps):\n",
    "    if game_type == 'contingency_game_shuffled':\n",
    "        axes.set_ylim([0, 6000])\n",
    "    elif game_type == 'change_agent_game':\n",
    "        axes.set_ylim([0, 750])\n",
    "    elif game_type in ['contingency_game', 'contingency_game_diff_color', 'contingency_game_0',\n",
    "                       ]:\n",
    "        if not last_100:\n",
    "            axes.set_ylim([0, 800])\n",
    "    elif game_type == \"contingency_game_shuffled_200\":\n",
    "        axes.set_ylim([0, 4000])\n",
    "    elif game_type in [\"contingency_game_shuffled_100\", \"contingency_game_shuffled_1\"]:\n",
    "        axes.set_ylim([0, 6000])\n",
    "        if game_type == \"contingency_game_shuffled_1\" and last_100:\n",
    "            axes.set_ylim([0, 2000])\n",
    "    elif game_type == 'logic_game':\n",
    "        axes.set_ylim([0, 100])\n",
    "    elif game_type == 'change_agent_game_harder_2':\n",
    "        axes.set_ylim([0, 200])\n",
    "\n",
    "    if perturbated and game_type in ['contingency_game']:\n",
    "        axes.set_ylim([0, 600])\n",
    "\n",
    "    if only_first_100 and ((\"self_class\" in agent_types) and (\"human\" in agent_types)):\n",
    "        axes.set_xlim([0, 120])\n",
    "        axes.set_ylim([0, 13]) if game_type == 'logic_game' else axes.set_ylim([0, 50])\n",
    "        if game_type == \"change_agent_game\":\n",
    "            axes.set_ylim([0, 160])\n",
    "\n",
    "    if only_first_100:\n",
    "        axes.set_xlim([0, 100])\n",
    "            \n",
    "\n",
    "    if only_first_100 and perturbated:\n",
    "        if game_type == \"change_agent_game\":\n",
    "            axes.set_xlim([0, 30])\n",
    "        else:\n",
    "            axes.set_xlim([0, 150])\n",
    "\n",
    "    if last_100 and game_type == \"contingency_game\":\n",
    "        axes.set_ylim([0, 50])\n",
    "    elif last_100 and game_type == \"change_agent_game\":\n",
    "        axes.set_ylim([0, 160])\n",
    "    elif only_first_100 and game_type in [\"change_agent_game\", \"change_agent_game_harder\"] and perturbated:\n",
    "        axes.set_ylim([0, 325])\n",
    "    elif last_100 and game_type == \"contingency_game_shuffled_100\":\n",
    "        axes.set_ylim([0, 2500])\n",
    "    elif (only_first_100 or last_100) and game_type == \"logic_game\":\n",
    "        axes.set_ylim([0, 20])\n",
    "    elif only_first_100 and game_type == 'contingency_game':\n",
    "        axes.set_ylim([0, 50])\n",
    "\n",
    "    y_str = \"No. Steps To Complete Level\" + '\\nAveraged Every {} Levels'.format(avg_per_n_steps)\n",
    "    if avg_per_n_steps == 1:\n",
    "        y_str = \"No. Steps To Complete Level\"\n",
    "\n",
    "    if game_type == \"contingency_game_shuffled_1\" and only_first_100:\n",
    "        axes.set_ylim([0, 50])\n",
    "\n",
    "    if not only_first_100 and not last_100:\n",
    "        ax.set_ylabel(ylabel=y_str, labelpad=21, fontsize=22)\n",
    "        ax.set_xlabel(xlabel=\"Levels Played\", labelpad=21, fontsize=22)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    else:\n",
    "        ax.tick_params(axis='both', which='major', labelsize=40)\n",
    "        if perturbated:\n",
    "            ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        \n",
    "\n",
    "def plot_error_line(err_mean):\n",
    "    SMALL_SIZE = 13 * 9/10\n",
    "    MEDIUM_SIZE = 20 * 9/10\n",
    "    BIGGER_SIZE = 24 * 9/10\n",
    "\n",
    "    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE + 1)  # fontsize of the figure title\n",
    "\n",
    "    xs = np.linspace(1, 9, num=9)\n",
    "    plt.figure()\n",
    "\n",
    "    kneedle = KneeLocator(xs, list(err_mean.values()), S=1.0, curve=\"convex\", direction=\"decreasing\")\n",
    "    print(\"Error elbow: \", kneedle.elbow)\n",
    "\n",
    "    plt.plot(xs, list(err_mean.values()))\n",
    "    plt.ylabel(\"Error\", fontweight='bold')\n",
    "    plt.xlabel(\"Degree\", fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    plt.savefig(\"./plots/err.png\", dpi=1000)\n",
    "\n",
    "# Finds the sf steps for artificial agents, based on the movement of the other agents\n",
    "def get_sf_steps(data, game_type):\n",
    "    sf_steps = []  # Containing self-finding steps for each level\n",
    "\n",
    "    if game_type != \"change_agent_game\":\n",
    "        for level in data['level']: # Iterate through all levels\n",
    "            cur_step = data['self_actions'][level]\n",
    "            possible_agents = [True] * 3  # Possible non selves (all are possible at first)\n",
    "\n",
    "            # Iterate through all steps in the level\n",
    "            for step in range(len(cur_step) - 2):\n",
    "                if step > 50:\n",
    "                    print(\"Warning: Step is greater than 50\")\n",
    "\n",
    "                # 0 = Up, 1 = Down, 2 = Left, 3 = Right\n",
    "                # Get non-self locations for the current level and the next level, and compare them\n",
    "                non_self_locs = data['ns_locs'][level][step]\n",
    "                next_non_self_locs = data['ns_locs'][level][step + 1]\n",
    "\n",
    "                # Iterate through all non selves, and mark the ones that are not possible\n",
    "                for i in range(len(non_self_locs)):\n",
    "                    if not possible_agents[i]:\n",
    "                        continue\n",
    "                    \n",
    "                    # Get the directions that each selves moved, i.e., if y axis increased by one, then it moved up\n",
    "                    # If [0] decreased by one, then it moved up, if [1] increased by one, then it moved right\n",
    "                    ns_move_dirs = [non_self_locs[i][0] - next_non_self_locs[i][0], non_self_locs[i][1] - next_non_self_locs[i][1]]\n",
    "\n",
    "                    # If the non self moved in a direction that is different from the command, then it is not possible\n",
    "                    if cur_step[step] == 0 and ns_move_dirs[0] != 1:\n",
    "                        possible_agents[i] = False\n",
    "                    elif cur_step[step] == 1 and ns_move_dirs[0] != -1:\n",
    "                        possible_agents[i] = False\n",
    "                    elif cur_step[step] == 2 and ns_move_dirs[1] != 1:\n",
    "                        possible_agents[i] = False\n",
    "                    elif cur_step[step] == 3 and ns_move_dirs[1] != -1:\n",
    "                        possible_agents[i] = False\n",
    "                \n",
    "                # If no non-selves are possible, then the self has found itself\n",
    "                # Add the step to the list of self-finding steps, and break out of the loop\n",
    "                if not any(possible_agents):\n",
    "                    sf_steps.append(step + 1)\n",
    "                    break\n",
    "    else: \n",
    "        # For the change agent game, calculate the self-finding steps each time the agent changes (i.e., when step % 7 == 0), and add it to the list.\n",
    "        # When the agent changes, reset the possible agents to all true\n",
    "        # When the agent finds itself, store the current step % 7 in the list that stores the self-finding steps for a given level\n",
    "        # When saving the self-finding steps, save the average in the level\n",
    "        for level in data['level']:\n",
    "            cur_step = data['self_actions'][level]\n",
    "            possible_agents = [True] * 3\n",
    "            level_sf_steps = []\n",
    "\n",
    "            step = 0\n",
    "            while True:\n",
    "                if step >= len(cur_step) - 2:\n",
    "                    break\n",
    "\n",
    "                if step % 7 == 0:\n",
    "                    possible_agents = [True] * 3\n",
    "\n",
    "                non_self_locs = data['ns_locs'][level][step]\n",
    "                next_non_self_locs = data['ns_locs'][level][step + 1]\n",
    "\n",
    "                for i in range(len(non_self_locs)):\n",
    "                    if not possible_agents[i]:\n",
    "                        continue\n",
    "                    \n",
    "                    ns_move_dirs = [non_self_locs[i][0] - next_non_self_locs[i][0], non_self_locs[i][1] - next_non_self_locs[i][1]]\n",
    "\n",
    "                    if cur_step[step] == 0 and ns_move_dirs[0] != 1:\n",
    "                        possible_agents[i] = False\n",
    "                    elif cur_step[step] == 1 and ns_move_dirs[0] != -1:\n",
    "                        possible_agents[i] = False\n",
    "                    elif cur_step[step] == 2 and ns_move_dirs[1] != 1:\n",
    "                        possible_agents[i] = False\n",
    "                    elif cur_step[step] == 3 and ns_move_dirs[1] != -1:\n",
    "                        possible_agents[i] = False\n",
    "                \n",
    "                # If no non-selves are possible, then the self has found itself\n",
    "                # Add the step to the level list of self-finding steps, and reset the possible agents\n",
    "                if not any(possible_agents):\n",
    "                    possible_agents = [True] * 3\n",
    "                    level_sf_steps.append((step + 1) % 7)\n",
    "\n",
    "                    # Skip to the next self-change step, e.g., if the self found itself at step 3, then skip to step 7\n",
    "                    step += 7 - (step % 7)\n",
    "                else:\n",
    "                    step += 1\n",
    "                \n",
    "            sf_steps.append(np.mean(level_sf_steps))\n",
    "    return sf_steps\n",
    "\n",
    "\n",
    "# avg_per_n_steps = Average per N levels\n",
    "# single_seed=True plots only a single seed (For testing)\n",
    "def get_performance(game_type, agent_types, avg_per_n_steps=10, single_seed=False, include_perturbation=False, find_elbow=False, save_data=False):\n",
    "    game_types = [game_type]\n",
    "    param_dict = {}; param_dict[game_type] = {}\n",
    "    \n",
    "    # Check if saved data exists\n",
    "    loaded = {}\n",
    "\n",
    "    if game_type != \"logic_game\" and save_data: agent_types.append(\"random_sf\")\n",
    "    for agent_type in agent_types:\n",
    "        loaded[agent_type] = False\n",
    "        fpath = './plotting_data/{}_'.format(game_type) + agent_type + \"{}\".format(\"_ext\" if include_perturbation else \"\") + \".pickle\"\n",
    "        if exists(fpath):\n",
    "            loaded[agent_type] = True\n",
    "            param_dict[game_type][agent_type] = pickle.load(open(fpath, 'rb'))\n",
    "            print(\"Loaded {}...\".format(agent_type))\n",
    "        else:\n",
    "            print(\"Will read {}...\".format(agent_type))\n",
    "            \n",
    "\n",
    "    if include_perturbation:\n",
    "        target_file_count = 40\n",
    "    else:\n",
    "        target_file_count = 20\n",
    "\n",
    "    target_file_count_o = target_file_count\n",
    "\n",
    "    ## ----- Read in data\n",
    "    for agent_type in agent_types:\n",
    "        if loaded[agent_type]:\n",
    "            continue\n",
    "        \n",
    "        if agent_type == \"random_sf\": agent_type = \"random\"\n",
    "        target_file_count = 2 if agent_type == \"self_classshort\" else target_file_count_o\n",
    "\n",
    "        files_o = glob.glob(\"../data/\" + game_type + \"/\" + agent_type + \"/*/*.json\")\n",
    "        if len(files_o) == 0:\n",
    "            files_o = glob.glob(\"../data/\" + game_type + \"/\" + agent_type + \"/*.json\")\n",
    "\n",
    "        files = files_o\n",
    "        # Skip stress test files\n",
    "        if not include_perturbation and agent_type not in [\"human\", \"data_sf\", \"human_extended\"]:\n",
    "            files = []\n",
    "            for x in files_o:\n",
    "                if 'short' in x:\n",
    "                    if int(x.split(\"/\")[-1][11:-5]) <= 1900:\n",
    "                        files.append(x)\n",
    "\n",
    "                else:\n",
    "                    if int(x.split(\"/\")[-1][6:-5]) <= 1900:\n",
    "                        files.append(x)\n",
    "\n",
    "        seed = 0\n",
    "        curr_file_count = 0\n",
    "        file_amt = len(files)\n",
    "        param_dict[game_type][agent_type] = {}\n",
    "        seed_current = []\n",
    "        all_seeds = []\n",
    "\n",
    "        # For saving self-finding steps of the agent\n",
    "        seed_current_sf = []\n",
    "        all_seeds_sf = []\n",
    "\n",
    "        seed_total = 20 if not single_seed else 1\n",
    "\n",
    "        if game_type in ['change_agent_game', 'change_agent_game_harder', 'change_agent_game_harder_2']:\n",
    "            seed_total = 18\n",
    "\n",
    "        if game_type in ['contingency_game_shuffled_100', 'contingency_game_shuffled_200']:\n",
    "            seed_total = 10\n",
    "\n",
    "        if single_seed:\n",
    "            seed_total = 1\n",
    "\n",
    "        sorted_files = sorted(files, key=os.path.getmtime if agent_type in ['human', 'data_sf',\n",
    "                                                                            'human_extended'] else get_seed_num_and_iter)\n",
    "        for i, file in enumerate(sorted_files):\n",
    "            with open(file, 'r') as fp:\n",
    "                print(\"Getting \", file, \"...\")\n",
    "                data = json.load(fp)\n",
    "                param_dict[game_type][agent_type] = data['data']['steps']\n",
    "                curr_file_count += 1\n",
    "\n",
    "                seed_current.append(data['data']['steps'])\n",
    "\n",
    "                if agent_type == \"random\": seed_current_sf.append(get_sf_steps(data['data'], game_type))\n",
    "\n",
    "                if agent_type in ['human', 'data_sf', 'human_extended'] and (file_amt == 1 or single_seed):\n",
    "                    param_dict[game_type][agent_type] = [data['data']['steps']]\n",
    "                    break\n",
    "                else:\n",
    "                    if agent_type in ['human', 'data_sf', 'human_extended'] and curr_file_count == file_amt:\n",
    "                        param_dict[game_type][agent_type] = seed_current\n",
    "                        break\n",
    "\n",
    "                if agent_type != 'human' and curr_file_count == target_file_count:\n",
    "                    all_seeds.append(seed_current)\n",
    "                    if agent_type == \"random\": all_seeds_sf.append(seed_current_sf); seed_current_sf = []\n",
    "                    \n",
    "                    seed_current = []\n",
    "                    curr_file_count = 0\n",
    "                    seed += 1\n",
    "\n",
    "                if seed == seed_total:\n",
    "                    print(\"Seed total: \", seed)\n",
    "                    print(\"Saving\")\n",
    "                    param_dict[game_type][agent_type] = all_seeds\n",
    "                    if agent_type == \"random\": param_dict[game_type][agent_type + \"_sf\"] = all_seeds_sf\n",
    "\n",
    "                    all_seeds = []\n",
    "                    all_seeds_sf = []\n",
    "\n",
    "                    if single_seed:\n",
    "                        break\n",
    "\n",
    "        # Save data of current agent\n",
    "        with open('./plotting_data/{}_'.format(game_type) + agent_type + \"{}\".format(\"_ext\" if include_perturbation else \"\") + \".pickle\", 'wb') as f:\n",
    "            print(\"Saving {}...\".format(agent_type)); pickle.dump(param_dict[game_type][agent_type], f)\n",
    "\n",
    "        # Save data of current agent's self-finding steps\n",
    "        if agent_type == \"random\":\n",
    "            with open('./plotting_data/{}_'.format(game_type) + agent_type + \"_sf\" + \"{}\".format(\"_ext\" if include_perturbation else \"\") + \".pickle\", 'wb') as f:\n",
    "                print(\"Saving {}...\".format(agent_type)); pickle.dump(param_dict[game_type][agent_type + \"_sf\"], f)\n",
    "        print(\"Passing from {} to Next agent\".format(agent_type))\n",
    "\n",
    "    ## ---- Get descriptive statistics\n",
    "    stats_dict = {}\n",
    "    stats_dict[game_type] = {}\n",
    "    for agent_type in agent_types:\n",
    "        print(agent_type)\n",
    "        raw_data = pd.DataFrame(param_dict[game_type][agent_type])\n",
    "\n",
    "        seed_average = []\n",
    "        seed_sem = []\n",
    "        for column in raw_data:\n",
    "            seed_average.append(np.mean(list(raw_data[column]), axis=0))\n",
    "            seed_sem.append((pd.DataFrame(list(raw_data[column]))).sem(axis=0))\n",
    "\n",
    "        curr_avg_data = pd.DataFrame(seed_average).T\n",
    "        curr_sem_data = pd.DataFrame(seed_sem).T\n",
    "        stats_dict[game_type][agent_type] = raw_data\n",
    "\n",
    "        # Fit a curve to human line, and use the elbow method to find the point of stabilization\n",
    "        if agent_type == \"human\":\n",
    "            errors = {}\n",
    "            def get_best_degree():\n",
    "                np.random.seed(0)\n",
    "                xs = list(range(0, 100))\n",
    "                ys = list(curr_avg_data.T[0])\n",
    "\n",
    "                from sklearn.linear_model import LinearRegression\n",
    "                from sklearn.preprocessing import PolynomialFeatures\n",
    "                from sklearn.metrics import mean_squared_error\n",
    "                from sklearn.model_selection import train_test_split\n",
    "\n",
    "                rmses = {} # Degree is key, RMSE is value\n",
    "                for i in range(0, 100):\n",
    "                    x_train, x_test, y_train, y_test = train_test_split(xs, ys, test_size=0.3)\n",
    "                    degrees = np.arange(1, 15)\n",
    "                    min_rmse, min_deg = 0, 0\n",
    "\n",
    "                    for deg in degrees:\n",
    "                        if deg not in list(rmses.keys()):\n",
    "                            rmses[deg] = []\n",
    "\n",
    "                        # Train features\n",
    "                        poly_features = PolynomialFeatures(degree=deg, include_bias=False)\n",
    "                        x_poly_train = poly_features.fit_transform(np.array(x_train).reshape(-1, 1))\n",
    "\n",
    "                        # Linear regression\n",
    "                        poly_reg = LinearRegression()\n",
    "                        poly_reg.fit(x_poly_train, y_train)\n",
    "\n",
    "                        # Compare with test data\n",
    "                        x_poly_test = poly_features.fit_transform(np.array(x_test).reshape(-1, 1))\n",
    "                        poly_predict = poly_reg.predict(x_poly_test)\n",
    "                        poly_mse = mean_squared_error(y_test, poly_predict)\n",
    "                        poly_rmse = np.sqrt(poly_mse)\n",
    "                        rmses[deg].append(poly_rmse)\n",
    "\n",
    "                # Plot and present results\n",
    "                min_rmse = statistics.mean(rmses[1])\n",
    "                min_deg = 1\n",
    "                    \n",
    "                rmse_means = []\n",
    "                for i in range(1, 15):\n",
    "                    if statistics.mean(rmses[i]) < min_rmse:\n",
    "                        min_deg = i\n",
    "                        min_rmse = statistics.mean(rmses[i])\n",
    "\n",
    "                    rmse_means.append(statistics.mean(rmses[i]))\n",
    "\n",
    "                print('Best degree {} with RMSE {}'.format(min_deg, min_rmse))\n",
    "                        \n",
    "                fig = plt.figure()\n",
    "                ax = fig.add_subplot(111)\n",
    "                ax.plot(degrees, rmse_means)\n",
    "                ax.set_yscale('log')\n",
    "                ax.set_xlabel('Degree')\n",
    "                ax.set_ylabel('RMSE')\n",
    "                plt.show()\n",
    "\n",
    "            def plot_curve(deg=20):\n",
    "                xs = list(range(0, 100))\n",
    "                ys = list(curr_avg_data.T[0])\n",
    "                fitted_poly = np.polyfit(xs, ys, deg=deg)\n",
    "\n",
    "                y_fitted = np.polyval(fitted_poly, xs)\n",
    "                kneedle = KneeLocator(xs, y_fitted, S=1.0, curve=\"convex\", direction=\"decreasing\")\n",
    "                print(kneedle.elbow)\n",
    "\n",
    "                plt.plot(xs, ys)\n",
    "                plt.plot(xs, y_fitted)\n",
    "                plt.show()\n",
    "                \n",
    "                plt.cla()\n",
    "                plt.clf()\n",
    "\n",
    "                kneedle.plot_knee()\n",
    "\n",
    "                plt.cla()\n",
    "                plt.clf()\n",
    "\n",
    "            if find_elbow:\n",
    "                plot_curve(deg=3)\n",
    "                get_best_degree()\n",
    "\n",
    "        if agent_type not in ['human', 'data_sf', 'human_extended']:  # Average per N levels for AI\n",
    "            stats_dict[game_type][agent_type + \"_m\"] = np.array(\n",
    "                [curr_avg_data[column].groupby(curr_avg_data.index // avg_per_n_steps).mean() for column in\n",
    "                    curr_avg_data]).reshape(\n",
    "                int(curr_avg_data.shape[1] * 100 * 1 / avg_per_n_steps))\n",
    "            stats_dict[game_type][agent_type + \"_se\"] = np.array(\n",
    "                [curr_sem_data[column].groupby(curr_sem_data.index // avg_per_n_steps).mean() for column in\n",
    "                    curr_sem_data]).reshape(\n",
    "                int(curr_sem_data.shape[1] * 100 * 1 / avg_per_n_steps))\n",
    "        else:  # Average per N levels for Human data\n",
    "            if agent_type in ['human', 'human_extended'] and include_perturbation and avg_per_n_steps != 1:\n",
    "                c_avg_per_n_steps = 1\n",
    "            else:\n",
    "                c_avg_per_n_steps = avg_per_n_steps\n",
    "                \n",
    "            temp = np.asarray(curr_avg_data).T\n",
    "            avg_ma = [temp[i:i + c_avg_per_n_steps].mean() for i in\n",
    "                        range(0, curr_avg_data.shape[1] - c_avg_per_n_steps + 1, c_avg_per_n_steps)]\n",
    "\n",
    "            temp = np.asarray(curr_sem_data).T\n",
    "            avg_se = [temp[i:i + c_avg_per_n_steps].mean() for i in\n",
    "                        range(0, curr_sem_data.shape[1] - c_avg_per_n_steps + 1, c_avg_per_n_steps)]\n",
    "\n",
    "            stats_dict[game_type][agent_type + \"_m\"] = np.array(avg_ma).reshape(\n",
    "                int(curr_avg_data.shape[1] / c_avg_per_n_steps))\n",
    "            stats_dict[game_type][agent_type + \"_se\"] = np.array(avg_se).reshape(\n",
    "                int(curr_sem_data.shape[1] / c_avg_per_n_steps))\n",
    "\n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "# perturbated=True: Plots the stress tests\n",
    "# only_first_100=True: Plots only the first hundred\n",
    "# last_100=True: Plots only the last hundred\n",
    "def plotter(stats, game_type, agent_types, avg_per_n_steps, perturbated=False, only_first_100=False, last_100=False, save_data=False):\n",
    "    st = stats[game_type]\n",
    "    \n",
    "    for i, a in enumerate(agent_types):\n",
    "        print(a)\n",
    "        \n",
    "        ## Saving data files for t-tests and BF's\n",
    "        if perturbated:  \n",
    "            if i == 0:\n",
    "                after_perturbation = {}          \n",
    "            if a not in ['human', 'data_sf', 'human_extended']:\n",
    "                after_perturbation[a + \"_all\"] = list(np.asarray(st[a].T.explode([i for i in range(0, st[a].shape[0])]).mean(axis=1).T))\n",
    "                after_perturbation[a + \"_averaged\"] = list(np.average(np.asarray(st[a].T.explode([i for i in range(0, st[a].shape[0])]).mean(axis=1)).reshape(-1, 40), axis=1))\n",
    "            else:\n",
    "                after_perturbation[\"human\"] = list(np.asarray(st[a].T.explode([i for i in range(0, st[a].shape[0])]).mean(axis=1).T))\n",
    "\n",
    "        else:\n",
    "            if i == 0:\n",
    "                jdata = {}\n",
    "\n",
    "            if a in ['human', 'data_sf', 'human_extended']:\n",
    "                jdata[a] = st[a].T.values.tolist()\n",
    "                jdata[a + '_avg'] = np.mean(st[a + '_m'])\n",
    "            else:\n",
    "                jdata[a + '_first_150'] = np.asarray([t[:150] for t in (\n",
    "                            list(st[a].iloc[:, 0:].values.T)[0] + list(st[a].iloc[:, 0:].values.T)[1])]).T.tolist()\n",
    "                jdata[a + '_first_150_avg'] = np.mean(st[a + '_m'][:150])\n",
    "                jdata[a + '_last_100'] = np.asarray([t[:100] for t in list(st[a].iloc[:, -1:].values.T)[0]]).T.tolist()\n",
    "                jdata[a + '_last_100_avg'] = np.mean(st[a + '_m'][-100:])\n",
    "\n",
    "                if game_type == \"logic_game\" and avg_per_n_steps == 1:\n",
    "                    jdata[a + '_all'] = list(st[a + '_m'])\n",
    "\n",
    "                # Save the self-finding steps for the random agent\n",
    "                if a == 'random' and game_type not in [\"logic_game\", \"contingency_game_shuffled_100\", \"contingency_game_shuffled_200\"] and save_data:\n",
    "                    jdata['random_sf_steps_first_150'] = np.asarray([t[:150] for t in list(st['random_sf'].iloc[:, 0:].values.T)[1]]).T.tolist()\n",
    "\n",
    "    if save_data and perturbated:\n",
    "        try:\n",
    "            with open('../stats/data_{}_after_perturbation.json'.format(game_type), 'w+') as fp:  # Save data files\n",
    "                json.dump(after_perturbation, fp, indent=4)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    if save_data and not perturbated:\n",
    "        try:\n",
    "            with open('../stats/data_{}.json'.format(game_type), 'w+') as fp:  # Save data files\n",
    "                print(\"*** Saving: \", jdata)\n",
    "                json.dump(jdata, fp, indent=4)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    if save_data:\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # Hide the right and top spines\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    axes = plt.gca()\n",
    "\n",
    "    ##### Cosmetic #####\n",
    "    cosmetic_change(game_type, ax, axes, last_100, agent_types, only_first_100, perturbated, avg_per_n_steps)\n",
    "\n",
    "    for j, agent_type in enumerate(agent_types):  # Plot each line\n",
    "        if agent_type not in ['human', 'data_sf', 'human_extended']:  # Algorithms\n",
    "\n",
    "            xs = [int(i * avg_per_n_steps) for i in range(0, len(st[agent_type + \"_m\"]))]\n",
    "            temp_xs = []\n",
    "            if last_100:\n",
    "                for a in xs:\n",
    "                    if a < 100:\n",
    "                        temp_xs.append(a)\n",
    "                xs = temp_xs\n",
    "                r_l = ax.plot(xs, st[agent_type + \"_m\"][-int(100 / avg_per_n_steps):], color=colors[agent_type],\n",
    "                              linewidth=4 if agent_type == \"self_class\" else 1,\n",
    "                              label=label_dict[agent_type])\n",
    "                ax.fill_between(xs, st[agent_type + \"_m\"][-int(100 / avg_per_n_steps):] - st[agent_type + \"_se\"][\n",
    "                                                                                     -int(100 / avg_per_n_steps):],\n",
    "                                st[agent_type + \"_m\"][-int(100 / avg_per_n_steps):] + st[agent_type + \"_se\"][\n",
    "                                                                                 -int(100 / avg_per_n_steps):],\n",
    "                                alpha=0.08, color=colors[agent_type])\n",
    "            else:\n",
    "                if \"self_class\" in agent_type:\n",
    "                    r_l = ax.plot(xs, st[agent_type + \"_m\"], color=colors[agent_type],\n",
    "                                  linewidth=4 if only_first_100 or last_100 else 2,\n",
    "                                  label=label_dict[agent_type])\n",
    "                else:\n",
    "                    r_l = ax.plot(xs, st[agent_type + \"_m\"], color=colors[agent_type], linewidth=1,\n",
    "                                  label=label_dict[agent_type])\n",
    "                ax.fill_between(xs, st[agent_type + \"_m\"] - st[agent_type + \"_se\"],\n",
    "                                st[agent_type + \"_m\"] + st[agent_type + \"_se\"], alpha=0.08,\n",
    "                                color=colors[agent_type])\n",
    "        else:  # Human\n",
    "            if agent_type in ['human', 'human_extended'] and perturbated and avg_per_n_steps != 1:\n",
    "                c_avg_per_n_steps = 1\n",
    "            else:\n",
    "                c_avg_per_n_steps = avg_per_n_steps\n",
    "            xs = [int(i * c_avg_per_n_steps) for i in range(0, len(st[\"{}_m\".format(agent_type)]))]\n",
    "            if last_100:\n",
    "                hum_l = ax.plot(xs, st['{}_m'.format(agent_type)], color=colors[agent_type], linewidth=2,\n",
    "                                label=label_dict[agent_type], linestyle='dashed', zorder=9999)\n",
    "            else:\n",
    "                hum_l = ax.plot(xs, st['{}_m'.format(agent_type)], color=colors[agent_type], linewidth=2,\n",
    "                                label=label_dict[agent_type], zorder=9999)\n",
    "            x = [int(i * c_avg_per_n_steps) for i in range(0, len(st[\"{}_m\".format(agent_type)]))]\n",
    "            ax.fill_between(x, st['{}_m'.format(agent_type)] - st['{}_se'.format(agent_type)],\n",
    "                            st['{}_m'.format(agent_type)] + st['{}_se'.format(agent_type)],\n",
    "                            alpha=0.08, color=colors[agent_type])\n",
    "    plt.tight_layout()\n",
    "    if perturbated:\n",
    "        if \"human_extended\" in agent_types:\n",
    "            if game_type == \"change_agent_game\":\n",
    "                if only_first_100:\n",
    "                    plt.axvline(x=20, color=\"#ffff03\", alpha=0.2, linewidth=15)\n",
    "                else:\n",
    "                    plt.axvline(x=2000, color=\"#ffff03\", alpha=0.2, linewidth=15)\n",
    "            else:\n",
    "                plt.axvline(x=100, color=\"#ffff03\", alpha=0.2, linewidth=15) if only_first_100 else plt.axvline(x=2000,\n",
    "                                                                                                                color=\"#ffff03\",\n",
    "                                                                                                                alpha=0.2,\n",
    "                                                                                                                linewidth=15)\n",
    "        else:\n",
    "            plt.axvline(x=2000, color=\"#ffff03\", alpha=0.2, linewidth=15)\n",
    "\n",
    "    #leg = plt.legend(bbox_to_anchor=(1, 1), loc='upper left', title_fontsize=25)\n",
    "    #plt.rc('legend', fontsize=25)\n",
    "\n",
    "    #for legobj in leg.legendHandles:\n",
    "    #    legobj.set_linewidth(6.0)\n",
    "    \n",
    "\n",
    "    # For plotting bayes lines\n",
    "    bfs = {}\n",
    "\n",
    "    try:\n",
    "        if only_first_100:\n",
    "            f = open(\"../stats/{}_bfs.json\".format(game_type))\n",
    "            bfs[game_type] = json.load(f)[game_type][\"perturbated\" if perturbated else \"normal\"]\n",
    "            bf = bfs[game_type]\n",
    "\n",
    "            for x, s in enumerate(bf):\n",
    "                if s == 1:\n",
    "                    if game_type == \"logic_game\":\n",
    "                        ax.hlines(xmin=max(0, x - 0.5), xmax=x + 0.5, y=17, linewidth=12, color=\"#0373fc\")\n",
    "                    elif game_type == \"contingency_game\":\n",
    "                        ax.hlines(xmin=max(0, x - 0.5), xmax=x + 0.5, y=45, linewidth=12, color=\"#0373fc\")\n",
    "                    elif game_type in [\"change_agent_game\", \"change_agent_game_harder\"]:\n",
    "                        if perturbated:\n",
    "                            ax.hlines(xmin=max(0, x - 0.5), xmax=x + 0.5, y=300, linewidth=12, color=\"#0373fc\")\n",
    "                        else:\n",
    "                            ax.hlines(xmin=max(0, x - 0.5), xmax=x + 0.5, y=150, linewidth=12, color=\"#0373fc\")\n",
    "                    elif game_type == \"contingency_game_shuffled_1\":\n",
    "                        ax.hlines(xmin=max(0, x - 0.5), xmax=x + 0.5, y=45, linewidth=12, color=\"#0373fc\")\n",
    "    except:\n",
    "        print(\"No bf\")\n",
    "\n",
    "    path = './plots/{}/curves/'.format(game_type)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # Human vs. Self\n",
    "    hs = ''\n",
    "    if only_first_100 and ((\"self_class\" in agent_types) and (\"human\" in agent_types)):\n",
    "        hs = '_human_vs_self'\n",
    "\n",
    "\n",
    "    if not save_data:\n",
    "        plt.show()\n",
    "        fig.savefig(\n",
    "            path + 'efficiency_curves_' + game_type + '{}{}{}{}{}.pdf'.format('_' + str(avg_per_n_steps),\n",
    "                                                                            '_first_100' if only_first_100 else '',\n",
    "                                                                            '_last_100' if last_100 else '',\n",
    "                                                                            '_perturbated' if perturbated else '',\n",
    "                                                                            hs),\n",
    "            format='pdf',\n",
    "            bbox_inches=\"tight\")\n",
    "\n",
    "# Append stats of two data (normal + include_perturbation)\n",
    "def append_stats(dict_normal, dict_ext, game_types, game_types_ext):\n",
    "    appended_data = {}\n",
    "    for i, game in enumerate(game_types):\n",
    "        appended_data[game] = {}\n",
    "        for key, value in dict_normal[game].items():\n",
    "            data_normal = dict_normal[game][key]\n",
    "            data_ext = dict_ext[game_types_ext[i]][key]\n",
    "            appended_data[game][key] = np.concatenate((data_normal, data_ext))\n",
    "\n",
    "    return appended_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### LOGIC GAME #########################\n",
    "\n",
    "game = \"logic_game\"\n",
    "\n",
    "agent_types = [\"self_class\", \"random\", \"human\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\"]\n",
    "avg_per_n_steps = 20\n",
    "\n",
    "# Save the data\n",
    "stats_dict = get_performance(game, agent_types, avg_per_n_steps, single_seed=False, include_perturbation=False)\n",
    "plotter(stats_dict, game, agent_types, avg_per_n_steps, perturbated=False, save_data=True)\n",
    "    \n",
    "# *-*-*-*-* 2000 Levels (Main Plot) *-*-*-*-* #\n",
    "stats_dict = get_performance(game, agent_types, avg_per_n_steps, single_seed=False, include_perturbation=False)\n",
    "plotter(stats_dict, game, agent_types, avg_per_n_steps, perturbated=False)\n",
    "\n",
    "# *-*-*-*-* Last Hundred *-*-*-*-* #\n",
    "plotter(stats_dict, game, [\"self_class\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\"], avg_per_n_steps, last_100=True)\n",
    "\n",
    "# *-*-*-*-* First Hundred *-*-*-*-* #\n",
    "agents_fs = [\"human\", \"self_class\"]\n",
    "plotter(get_performance(game, agents_fs, 1, single_seed=False), game, agents_fs, 1, only_first_100=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### CONTINGENCY GAME #########################\n",
    "\n",
    "game = \"contingency_game\"\n",
    "\n",
    "# For saving data\n",
    "agents = [\"random\", \"human\", \"human_extended\", \"data_sf\", \"self_classshort\", \"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"a2c_training\", \"option_critic\"]\n",
    "stats = get_performance(game, agents, avg_per_n_steps=20, single_seed=False,\n",
    "                            include_perturbation=False, save_data=True)\n",
    "plotter(stats, game, agents, avg_per_n_steps=20, perturbated=False, save_data=True)\n",
    "\n",
    "# *-*-*-*-* 2000 Levels (Main Plot) *-*-*-*-* #\n",
    "agent_types = [\"self_class\", \"random\", \"human\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\"]\n",
    "avg_per_n_steps = 20\n",
    "stats_dict = get_performance(game, agent_types, avg_per_n_steps, single_seed=False, include_perturbation=False)\n",
    "plotter(stats_dict, game, agent_types, avg_per_n_steps, perturbated=False)\n",
    "\n",
    "# *-*-*-*-* Last Hundred *-*-*-*-* #\n",
    "plotter(stats_dict, game, [\"self_class\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\"], avg_per_n_steps, last_100=True)\n",
    "\n",
    "# *-*-*-*-* First Hundred *-*-*-*-* #\n",
    "agents_fs = [\"human\", \"self_class\"]\n",
    "plotter(get_performance(game, agents_fs, 1, single_seed=False), game, agents_fs, 1, only_first_100=True)\n",
    "        \n",
    "# *-*-*-*-* Perturbations *-*-*-*-* #\n",
    "if game not in [\"logic_game\", \"contingency_game_shuffled_1\"]:  # These games do not have perturbations\n",
    "    # Save data\n",
    "    agents_stress = [\"human_extended\", \"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\", \"a2c_training\"]\n",
    "    stress_stats = get_performance(game, agents_stress, avg_per_n_steps, single_seed=False, include_perturbation=True)\n",
    "    plotter(stress_stats, game, agents_stress, avg_per_n_steps, perturbated=True, save_data=True)\n",
    "\n",
    "    # Plot\n",
    "    agents_stress = [\"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\", \"a2c_training\"]\n",
    "    stress_stats = get_performance(game, agents_stress, avg_per_n_steps, single_seed=False, include_perturbation=True)\n",
    "    plotter(stress_stats, game, agents_stress, avg_per_n_steps, perturbated=True)\n",
    "\n",
    "    agents_stress = [\"self_classshort\", \"human_extended\"]\n",
    "    stress_stats = get_performance(game, agents_stress, avg_per_n_steps=1, single_seed=False,\n",
    "                                include_perturbation=True)\n",
    "    plotter(stress_stats, game, agents_stress, avg_per_n_steps=1, perturbated=True, only_first_100=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### SWITCHING MAPPINGS GAME #########################\n",
    "\n",
    "game = \"contingency_game_shuffled_1\"  # \"logic_game\", \"contingency_game_shuffled_1\", \"change_agent_game\",\n",
    "\n",
    "# For saving data\n",
    "agents = [\"human\", \"random\", \"random_sf\", \"human_extended\", \"data_sf\", \"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"a2c_training\", \"option_critic\"]\n",
    "stats = get_performance(game, agents, avg_per_n_steps=20, single_seed=False,\n",
    "                            include_perturbation=False)\n",
    "\n",
    "plotter(stats, game, agents, avg_per_n_steps=20, perturbated=False, save_data=True)\n",
    "agent_types = [\"self_class\", \"random\", \"human\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\"]\n",
    "\n",
    "avg_per_n_steps = 20\n",
    "    \n",
    "# *-*-*-*-* 2000 Levels (Main Plot) *-*-*-*-* #\n",
    "stats_dict = get_performance(game, agent_types, avg_per_n_steps, single_seed=False, include_perturbation=False)\n",
    "plotter(stats_dict, game, agent_types, avg_per_n_steps, perturbated=False)\n",
    "\n",
    "# *-*-*-*-* Last Hundred *-*-*-*-* #\n",
    "plotter(stats_dict, game, [\"self_class\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\"], avg_per_n_steps, last_100=True)\n",
    "\n",
    "# *-*-*-*-* First Hundred *-*-*-*-* #\n",
    "agents_fs = [\"human\", \"self_class\"]\n",
    "plotter(get_performance(game, agents_fs, 1, single_seed=False), game, agents_fs, 1, only_first_100=True)\n",
    "        \n",
    "# *-*-*-*-* Perturbations *-*-*-*-* #\n",
    "if game not in [\"logic_game\", \"contingency_game_shuffled_1\"]:  # These games do not have perturbations\n",
    "    agents_stress = [\"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\", \"a2c_training\"]\n",
    "    stress_stats = get_performance(game, agents_stress, avg_per_n_steps, single_seed=False, include_perturbation=True)\n",
    "    plotter(stress_stats, game, agents_stress, avg_per_n_steps, perturbated=True)\n",
    "\n",
    "    agents_stress = [\"self_classshort\", \"human_extended\"]\n",
    "    stress_stats = get_performance(game, agents_stress, avg_per_n_steps=1, single_seed=False,\n",
    "                                include_perturbation=True)\n",
    "    plotter(stress_stats, game, agents_stress, avg_per_n_steps=1, perturbated=True, only_first_100=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less frequently shuffled contingency games\n",
    "\n",
    "game = \"contingency_game_shuffled_100\"  # \"logic_game\", \"contingency_game_shuffled_1\", \"change_agent_game\",\n",
    "agent_types = [\"self_class\", \"random\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\"]\n",
    "avg_per_n_steps = 20\n",
    "    \n",
    "# *-*-*-*-* 2000 Levels (Main Plot) *-*-*-*-* #\n",
    "stats_dict = get_performance(game, agent_types, avg_per_n_steps, single_seed=False, include_perturbation=False)\n",
    "plotter(stats_dict, game, agent_types, avg_per_n_steps, perturbated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less frequently shuffled contingency games\n",
    "\n",
    "game = \"contingency_game_shuffled_200\"  # \"logic_game\", \"contingency_game_shuffled_1\", \"change_agent_game\",\n",
    "agent_types = [\"self_class\", \"random\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\"]\n",
    "avg_per_n_steps = 20\n",
    "    \n",
    "# *-*-*-*-* 2000 Levels (Main Plot) *-*-*-*-* #\n",
    "stats_dict = get_performance(game, agent_types, avg_per_n_steps, single_seed=False, include_perturbation=False)\n",
    "plotter(stats_dict, game, agent_types, avg_per_n_steps, perturbated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "######################### SWITCHING EMBODIMENTS GAME #########################\n",
    "\n",
    "game = \"change_agent_game\"  # \"logic_game\", \"contingency_game_shuffled_1\", \"change_agent_game\",\n",
    "\n",
    "# For saving data\n",
    "agents = [\"human\", \"random\", \"human_extended\", \"random_sf\", \"data_sf\", \"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"a2c_training\", \"keep_close\", \"option_critic\"]\n",
    "stats = get_performance(game, agents, avg_per_n_steps=20, single_seed=False,\n",
    "                            include_perturbation=False)\n",
    "plotter(stats, game, agents, avg_per_n_steps=20, perturbated=False, save_data=True)\n",
    "\n",
    "\n",
    "agents = [\"human\", \"random\", \"human_extended\", \"random_sf\", \"data_sf\", \"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"a2c_training\", \"keep_close\", \"option_critic\"]\n",
    "stats = get_performance(game, agents, avg_per_n_steps=1, single_seed=False,\n",
    "                            include_perturbation=False)\n",
    "plotter(stats, game, agents, avg_per_n_steps=1, perturbated=True, save_data=True)\n",
    "\n",
    "agent_types = [\"self_class\", \"random\", \"human\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\", \"keep_close\"]  # 'random'\n",
    "\n",
    "avg_per_n_steps = 20\n",
    "    \n",
    "# *-*-*-*-* 2000 Levels (Main Plot) *-*-*-*-* #\n",
    "stats_dict = get_performance(game, agent_types, avg_per_n_steps, single_seed=False, include_perturbation=False)\n",
    "plotter(stats_dict, game, agent_types, avg_per_n_steps, perturbated=False)\n",
    "\n",
    "# *-*-*-*-* Last Hundred *-*-*-*-* #\n",
    "plotter(stats_dict, game, [\"self_class\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\", \n",
    "                            \"keep_close\"], avg_per_n_steps, last_100=True)\n",
    "\n",
    "# *-*-*-*-* First Hundred *-*-*-*-* #\n",
    "agents_fs = [\"human\", \"self_class\"]\n",
    "plotter(get_performance(game, agents_fs, 1, single_seed=False), game, agents_fs, 1, only_first_100=True)\n",
    "        \n",
    "# *-*-*-*-* Perturbations *-*-*-*-* #\n",
    "if game not in [\"logic_game\", \"contingency_game_shuffled_1\"]:  # These games do not have perturbations\n",
    "    agents_stress = [\"keep_close\", \"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"option_critic\", \"a2c_training\"]\n",
    "    stress_stats = get_performance(game, agents_stress, avg_per_n_steps, single_seed=False, include_perturbation=True)\n",
    "    plotter(stress_stats, game, agents_stress, avg_per_n_steps, perturbated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################  HARDER PERTURBATION TASK IN SWITCHING EMBODIMENTS GAME  #########################################\n",
    "\n",
    "# Save data file for stats\n",
    "agents_stress = [\"human_extended\", \"data_sf\", \"self_classshort\", \"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"a2c_training\", \"keep_close\"]\n",
    "stress_stats = get_performance('change_agent_game_harder', agents_stress, avg_per_n_steps=20, single_seed=False,\n",
    "                            include_perturbation=True)\n",
    "plotter(stress_stats, 'change_agent_game_harder', agents_stress, avg_per_n_steps=20, perturbated=False, save_data=True)\n",
    "\n",
    "agents_stress = [\"human_extended\", \"self_classshort\", \"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"a2c_training\", \"keep_close\"]\n",
    "stress_stats = get_performance('change_agent_game_harder', agents_stress, avg_per_n_steps=1, single_seed=False,\n",
    "                            include_perturbation=True)\n",
    "plotter(stress_stats, 'change_agent_game_harder', agents_stress, avg_per_n_steps=1, perturbated=True, save_data=True)\n",
    "\n",
    "# Harder Stress Test for Switching Embodiments Game:\n",
    "agents_stress = [\"self_class\", \"trpo_training\", \"acer_training\", \"ppo2_training\", \"dqn_training\", \"a2c_training\", \"keep_close\"]\n",
    "stress_stats = get_performance('change_agent_game_harder', agents_stress, avg_per_n_steps=20, single_seed=False,\n",
    "                            include_perturbation=True)\n",
    "plotter(stress_stats, 'change_agent_game_harder', agents_stress, avg_per_n_steps=20, perturbated=True, save_data=False)\n",
    "\n",
    "# Harder Stress Test for human participants\n",
    "agents_stress = [\"self_classshort\", \"human_extended\"]\n",
    "stress_stats = get_performance('change_agent_game_harder', agents_stress, avg_per_n_steps=1, single_seed=False,\n",
    "                               include_perturbation=True)\n",
    "plotter(stress_stats, 'change_agent_game_harder', agents_stress, avg_per_n_steps=1, perturbated=True, only_first_100=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switching Embodiments 2 (Figure S10)\n",
    "\n",
    "avg_per_n_steps = 20\n",
    "game = \"change_agent_game_harder_2\"\n",
    "agents_stress = [\"keep_close\", \"self_class\", \"acer_training\"]\n",
    "stress_stats = get_performance(game, agents_stress, avg_per_n_steps, single_seed=False, include_perturbation=True)\n",
    "plotter(stress_stats, game, agents_stress, avg_per_n_steps, perturbated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----- Plot the number of steps the player takes before any agent moves from its starting location\n",
    "\n",
    "# Plot no-movement action count of agent for each level\n",
    "def plot_nm_ac(game_types, agent_types, avg_per_n_steps=20, only_first_100=False):\n",
    "    for i, game_type in enumerate(game_types):\n",
    "        nm_ac = get_all_nm_ac(agent_types, game_type, avg_per_n_steps)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "        ax.xaxis.label.set_size(20)\n",
    "        ax.yaxis.label.set_size(20)\n",
    "\n",
    "        if only_first_100:\n",
    "            ax.set_xlim([0, 100])\n",
    "\n",
    "        ax.set_xlabel(xlabel=\"Levels Played\", labelpad=21)\n",
    "\n",
    "        ax.set_ylabel(ylabel=\"No. Steps Until Self Orienting\\nAveraged Every {} Levels\".format(avg_per_n_steps),\n",
    "                      labelpad=21)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "        for j, agent in enumerate(agent_types):\n",
    "            xs = [int(i * avg_per_n_steps) for i in range(0, len(nm_ac[agent + \"_m\"]))]\n",
    "            line_width = 3 if agent == 'human' else 1\n",
    "            ax.plot(xs, nm_ac[agent + \"_m\"], color=colors[agent], linewidth=line_width, label=label_dict[agent])\n",
    "            upper_curve = [nm_ac[agent + '_m'][i] + nm_ac[agent + '_se'][i] for i in\n",
    "                           range(len(nm_ac[agent + '_m']))]\n",
    "            lower_curve = [nm_ac[agent + '_m'][i] - nm_ac[agent + '_se'][i] for i in\n",
    "                           range(len(nm_ac[agent + '_m']))]\n",
    "            x = [int(i * avg_per_n_steps) for i in range(0, len(nm_ac[agent + \"_m\"]))]\n",
    "            ax.fill_between(x, lower_curve, upper_curve, alpha=0.05, color=colors[agent])\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        #plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left', fontsize='xx-large')\n",
    "\n",
    "        fs = 50 if 'contingency_game' in game_type else 25\n",
    "        fs = 100 if 'contingency_game_shuffled' in game_type else fs\n",
    "\n",
    "        #leg = plt.legend(bbox_to_anchor=(1, 1), loc='upper left', title_fontsize=fs)\n",
    "        #plt.rc('legend', fontsize=fs)\n",
    "\n",
    "        #for legobj in leg.legendHandles:\n",
    "        #    legobj.set_linewidth(6.0)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        path = './plots/{}/{}/'.format(game_type, 'curves')\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        fig.savefig(path + 'no_movement_action_count_{}{}.pdf'.format(game_type, avg_per_n_steps), format='pdf')\n",
    "\n",
    "\n",
    "## Get no-movement action count of agent for each level and each game as a dictionary\n",
    "def get_all_nm_ac(agent_types, game_type, avg_per_n_steps):\n",
    "    # Check if saved data exists\n",
    "    fpath = './plotting_data/nm_ac_' + game_type + \"_batchSize=\" + str(avg_per_n_steps) + \".pickle\"\n",
    "    if exists(fpath):\n",
    "        return pickle.load(open(fpath, 'rb'))\n",
    "\n",
    "    stats = {}\n",
    "\n",
    "    df = None\n",
    "    for agent in agent_types:\n",
    "        stats[agent + \"_m\"], stats[agent + \"_se\"], df_agent = get_nm_ac(agent, game_type, avg_per_n_steps)\n",
    "\n",
    "        if df is None:\n",
    "            df = df_agent\n",
    "            df = df.rename(columns={\"self_finding_steps\": agent + \"_self_finding_steps\", \"steps\": agent + \"_total_steps\"})\n",
    "        else:\n",
    "            df[agent + \"_self_finding_steps\"] = df_agent[\"self_finding_steps\"]\n",
    "            df[agent + \"_total_steps\"] = df_agent[\"steps\"]\n",
    "\n",
    "    # Save data\n",
    "    with open(fpath, 'wb') as f:\n",
    "        pickle.dump(stats, f)\n",
    "\n",
    "    df.to_csv('../stats/self_orienting_logic_game.csv')\n",
    "    return stats\n",
    "\n",
    "\n",
    "## Get no-movement action count of agent for each level\n",
    "def get_nm_ac(agent_type, game_type, avg_per_n_steps):\n",
    "    files = glob.glob(\"../data/\" + game_type + \"/\" + agent_type + \"/*/*.json\")\n",
    "\n",
    "    if len(files) == 0:\n",
    "        files = glob.glob(\"../data/\" + game_type + \"/\" + agent_type + \"/*.json\")\n",
    "\n",
    "    # Storing the total steps\n",
    "    all_seeds_steps = []\n",
    "    curr_seed_steps = []\n",
    "\n",
    "    # Storing the action count until displacement\n",
    "    all_seeds = []\n",
    "    curr_seed = []\n",
    "    seed = 0\n",
    "    curr_file_count = 0\n",
    "    file_amt = len(files)\n",
    "\n",
    "    if agent_type not in [\"human\", \"random\"]:\n",
    "        files = [x for x in files if int(x.split(\"/\")[-1][6:-5]) <= 1900]\n",
    "\n",
    "    sorted_files = sorted(files, key=os.path.getmtime if agent_type == 'human' else get_seed_num_and_iter)\n",
    "    for i, file in enumerate(sorted_files):\n",
    "        print(\"Getting... \", file)\n",
    "        data = json.load(open(file))\n",
    "        self_locs = data.get(\"data\")[\"self_locs\"]\n",
    "\n",
    "        level_amt = 100\n",
    "        action_count = [1] * level_amt\n",
    "\n",
    "        # In each 100 levels:\n",
    "        for level in range(level_amt):\n",
    "            if len(self_locs[level]) == 0:\n",
    "                continue\n",
    "            action_amt = len(self_locs[level][0])\n",
    "            for i in range(action_amt):\n",
    "                if i == level_amt:\n",
    "                    break\n",
    "\n",
    "                x = self_locs[level][0][i]\n",
    "                y = self_locs[level][1][i]\n",
    "                x1 = self_locs[level][0][i + 1]\n",
    "                y1 = self_locs[level][1][i + 1]\n",
    "\n",
    "                if x == x1 and y == y1:  # Still in the same position\n",
    "                    action_count[level] = action_count[level] + 1\n",
    "                else:  # Position have changed\n",
    "                    break\n",
    "\n",
    "        curr_file_count += 1\n",
    "        curr_seed.append(action_count)\n",
    "        curr_seed_steps.append(data.get(\"data\")[\"steps\"])\n",
    "\n",
    "        if agent_type == 'human' and (curr_file_count == file_amt or file_amt == 1):\n",
    "            all_seeds = curr_seed\n",
    "            all_seeds_steps = curr_seed_steps\n",
    "            break\n",
    "\n",
    "        if agent_type != 'human' and curr_file_count == 20:\n",
    "            all_seeds.append(curr_seed)\n",
    "            all_seeds_steps.append(curr_seed_steps)\n",
    "\n",
    "            curr_seed = []\n",
    "            curr_seed_steps = []\n",
    "\n",
    "            curr_file_count = 0\n",
    "            seed += 1\n",
    "\n",
    "    all_seeds = pd.DataFrame(all_seeds)\n",
    "    data_long = all_seeds.T\n",
    "\n",
    "    if agent_type != 'human':\n",
    "        data_long = data_long.explode([i for i in range(0, 20)])\n",
    "\n",
    "    data_long = data_long.values.reshape(-1)\n",
    "    participant = [i % 20 for i in range(0, 40000)]\n",
    "    level = [math.floor(i / 20) for i in range(0, 40000)]\n",
    "\n",
    "    if agent_type == 'human':\n",
    "        rem = [0 for i in range(0, 38000)]\n",
    "        data_long = np.append(data_long, rem)\n",
    "\n",
    "    all_seeds_steps = pd.DataFrame(all_seeds_steps)\n",
    "    data_long_steps = all_seeds_steps.T\n",
    "\n",
    "    if agent_type != 'human':\n",
    "        data_long_steps = data_long_steps.explode([i for i in range(0, 20)])\n",
    "    \n",
    "    data_long_steps = data_long_steps.values.reshape(-1)\n",
    "    if agent_type == 'human':\n",
    "        rem = [0 for i in range(0, 38000)]\n",
    "        data_long_steps = np.append(data_long_steps, rem)\n",
    "\n",
    "    all_data = pd.DataFrame.from_dict({'self_finding_steps': data_long, 'steps': data_long_steps, 'participant': participant, 'level': level})\n",
    "\n",
    "    seed_average = []\n",
    "    seed_sem = []\n",
    "    for column in all_seeds:\n",
    "        seed_average.append(np.mean(list(all_seeds[column]), axis=0))\n",
    "        seed_sem.append((pd.DataFrame(list(all_seeds[column]))).sem(axis=0))\n",
    "\n",
    "    curr_avg_data = pd.DataFrame(seed_average).T\n",
    "    curr_sem_data = pd.DataFrame(seed_sem).T\n",
    "\n",
    "    if agent_type != 'human':\n",
    "        seed_mean = np.array(\n",
    "            [curr_avg_data[column].groupby(curr_avg_data.index // avg_per_n_steps).mean() for column in\n",
    "             curr_avg_data]).reshape(\n",
    "            int(curr_avg_data.shape[1] * 100 * 1 / avg_per_n_steps))\n",
    "        s_sem = np.array(\n",
    "            [curr_sem_data[column].groupby(curr_sem_data.index // avg_per_n_steps).mean() for column in\n",
    "             curr_sem_data]).reshape(\n",
    "            int(curr_sem_data.shape[1] * 100 * 1 / avg_per_n_steps))\n",
    "    else:\n",
    "        temp = np.asarray(curr_avg_data).T\n",
    "        avg_ma = [temp[i:i + avg_per_n_steps].mean() for i in range(0, curr_avg_data.shape[1] - avg_per_n_steps + 1, avg_per_n_steps)]\n",
    "\n",
    "        temp = np.asarray(curr_sem_data).T\n",
    "        avg_se = [temp[i:i + avg_per_n_steps].mean() for i in range(0, curr_sem_data.shape[1] - avg_per_n_steps + 1, avg_per_n_steps)]\n",
    "\n",
    "        seed_mean = np.array(avg_ma).reshape(int(curr_avg_data.shape[1] / avg_per_n_steps))\n",
    "        s_sem = np.array(avg_se).reshape(int(curr_sem_data.shape[1] / avg_per_n_steps))\n",
    "\n",
    "    return seed_mean, s_sem, all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Action Counts Without Movement -- Only For Logic Game\n",
    "\n",
    "agent_types = [\"human\", \"self_class\", \"random\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\",\n",
    "               \"dqn_training\", \"option_critic\"]\n",
    "game_types = [\"logic_game\"]\n",
    "plot_nm_ac(game_types, agent_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ----- Plot average distance from the goal for each level\n",
    "def plot_avg_distance(game_types, agent_types, avg_per_n_steps=50, level=1999):\n",
    "    for i, game_type in enumerate(game_types):\n",
    "        avg_distance = get_all_avg_distance(agent_types, game_type, avg_per_n_steps, level)\n",
    "        xls = [150]\n",
    "        for xl in xls:\n",
    "            fig, ax = plt.subplots(figsize=(20, 10))\n",
    "            ax.set_xlim([0, xl])\n",
    "            ax.xaxis.label.set_size(20)\n",
    "            ax.yaxis.label.set_size(20)\n",
    "\n",
    "            #ax.set_xlabel(xlabel=\"Step\", labelpad=21)\n",
    "            #ax.set_ylabel(ylabel=\"Average Distance to Goal\", labelpad=21)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=17)\n",
    "            ax.xaxis.set_tick_params(labelsize=20)\n",
    "            ax.yaxis.set_tick_params(labelsize=20)\n",
    "            #ax.set_title(game_titles[game_type], fontweight='bold', fontsize=25)\n",
    "\n",
    "            for j, agent in enumerate(agent_types):\n",
    "                xs = [int(i * avg_per_n_steps) for i in range(0, len(avg_distance[agent + \"_m\"]))]\n",
    "                line_width = 4 if agent == 'human' or agent == 'self_class' else 1\n",
    "                ax.plot(xs, avg_distance[agent + \"_m\"], color=colors[agent], linewidth=line_width,\n",
    "                        label=label_dict[agent])\n",
    "                upper_curve = [avg_distance[agent + '_m'][i] + avg_distance[agent + '_se'][i] for i in\n",
    "                               range(len(avg_distance[agent + '_m']))]\n",
    "                lower_curve = [avg_distance[agent + '_m'][i] - avg_distance[agent + '_se'][i] for i in\n",
    "                               range(len(avg_distance[agent + '_m']))]\n",
    "                x = [int(i * avg_per_n_steps) for i in range(0, len(avg_distance[agent + \"_m\"]))]\n",
    "                ax.fill_between(x, lower_curve, upper_curve, alpha=0.08, color=colors[agent])\n",
    "\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            path = './plots/{}/{}/'.format(game_type, 'curves')\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "            fig.savefig(path + '{}_distance_{}{}{}.pdf'.format(xl, game_type, avg_per_n_steps, level), format='pdf')\n",
    "\n",
    "\n",
    "def get_all_avg_distance(agent_types, game_type, avg_per_n_steps, level=0):\n",
    "    # Check if saved data exists\n",
    "    fpath = './plotting_data/dist_' + game_type + \"_batchSize=\" + str(avg_per_n_steps) + \"level={}\".format(\n",
    "        str(level)) + \".pickle\"\n",
    "    if exists(fpath):\n",
    "        return pickle.load(open(fpath, 'rb'))\n",
    "\n",
    "    stats = {}\n",
    "    for agent in agent_types:\n",
    "        print(agent + \"_m\")\n",
    "        stats[agent + \"_m\"], stats[agent + \"_se\"] = get_avg_distance(agent, game_type, avg_per_n_steps, level)\n",
    "\n",
    "    # Save data\n",
    "    with open(fpath, 'wb') as f:\n",
    "        pickle.dump(stats, f)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# Ignore avg_per_n_steps here, as it does not mean anything\n",
    "def get_avg_distance(agent_type, game_type, avg_per_n_steps, level=0):\n",
    "    if avg_per_n_steps != 1:\n",
    "        print(\"Batch size should be 1\")\n",
    "        return\n",
    "    files = glob.glob(\"../data/\" + game_type + \"/\" + agent_type + \"/*/*.json\")\n",
    "\n",
    "    if len(files) == 0:\n",
    "        files = glob.glob(\"../data/\" + game_type + \"/\" + agent_type + \"/*.json\")\n",
    "\n",
    "    all_seeds = []\n",
    "    curr_file_count = 0\n",
    "\n",
    "    if agent_type not in [\"human\"]:\n",
    "        files = [x for x in files if int(x.split(\"/\")[-1][6:-5]) <= 1900]\n",
    "\n",
    "    sorted_files = sorted(files, key=os.path.getmtime if agent_type == 'human' else get_seed_num_and_iter)\n",
    "\n",
    "    if agent_type != 'human':\n",
    "        if level == 1:\n",
    "            sorted_files = [sorted_files[j] for j in range(0, len(files), 20)]\n",
    "        elif level == 1999:  # Get the files that only contain the last level\n",
    "            sf = []\n",
    "            for j in range(0, len(files), 20):\n",
    "                sf.append(sorted_files[j - 1])\n",
    "            sorted_files = sf\n",
    "\n",
    "    file_amt = len(sorted_files)\n",
    "    level = level % 100\n",
    "\n",
    "    # For each seed/subject:\n",
    "    for i, file in enumerate(sorted_files):\n",
    "        print(\"Getting... \", file)\n",
    "        data = json.load(open(file))\n",
    "        self_locs = data.get(\"data\")[\"self_locs\"]\n",
    "\n",
    "        #level_amt = 100\n",
    "        action_amt = len(self_locs[level][0])\n",
    "\n",
    "        # Store distance to goal at every action for a particular seed\n",
    "        distances = [0 for y in range(action_amt)]\n",
    "\n",
    "        if len(self_locs[level]) == 0:\n",
    "            continue\n",
    "\n",
    "        # In each action:\n",
    "        for action_index in range(action_amt):\n",
    "            x = self_locs[level][0][action_index]\n",
    "            y = self_locs[level][1][action_index]\n",
    "\n",
    "            if x == 0:\n",
    "                print(\"?\")\n",
    "\n",
    "            if game_type == \"logic_game\":\n",
    "                distances[action_index] = abs(x - 4) + abs(y - 4)\n",
    "\n",
    "                if (abs(x - 4) + abs(y - 4)) > 6:\n",
    "                    print(\"how?\")\n",
    "            else:\n",
    "                distances[action_index] = abs(x - 10) + abs(y - 10)\n",
    "\n",
    "        curr_file_count += 1\n",
    "        all_seeds.append(distances)\n",
    "\n",
    "    all_seeds = pd.DataFrame(all_seeds).fillna(0)\n",
    "\n",
    "    if agent_type == 'human':\n",
    "        if game_type == \"logic_game\":\n",
    "            all_seeds.insert(loc=0, value=pd.DataFrame([6 for i in range(file_amt)]),\n",
    "                            column=-1)  # Set starting distance (8)\n",
    "        else:\n",
    "            all_seeds.insert(loc=0, value=pd.DataFrame([8 for i in range(file_amt)]),\n",
    "                            column=-1)  # Set starting distance (8)\n",
    "\n",
    "    seed_average = []\n",
    "    seed_sem = []\n",
    "    for column in all_seeds:\n",
    "        seed_average.append(np.mean(list(all_seeds[column]), axis=0))\n",
    "        seed_sem.append((pd.DataFrame(list(all_seeds[column]))).sem(axis=0))\n",
    "\n",
    "    curr_avg_data = pd.DataFrame(seed_average).T\n",
    "    curr_sem_data = pd.DataFrame(seed_sem).T\n",
    "\n",
    "    if agent_type != 'human':  # AI\n",
    "        seed_mean = np.array(\n",
    "            [curr_avg_data[column].groupby(curr_avg_data.index // avg_per_n_steps).mean() for column in\n",
    "             curr_avg_data]).reshape(\n",
    "            int(curr_avg_data.shape[1] * 1 / avg_per_n_steps))\n",
    "        s_sem = np.array(\n",
    "            [curr_sem_data[column].groupby(curr_sem_data.index // avg_per_n_steps).mean() for column in\n",
    "             curr_sem_data]).reshape(\n",
    "            int(curr_sem_data.shape[1] * 1 / avg_per_n_steps))\n",
    "    else:\n",
    "        temp = np.asarray(curr_avg_data).T\n",
    "        avg_ma = [temp[i:i + avg_per_n_steps].mean() for i in range(0, curr_avg_data.shape[1] - avg_per_n_steps + 1, avg_per_n_steps)]\n",
    "\n",
    "        temp = np.asarray(curr_sem_data).T\n",
    "        avg_se = [temp[i:i + avg_per_n_steps].mean() for i in range(0, curr_sem_data.shape[1] - avg_per_n_steps + 1, avg_per_n_steps)]\n",
    "\n",
    "        seed_mean = np.array(avg_ma).reshape(int(curr_avg_data.shape[1] / avg_per_n_steps))\n",
    "        s_sem = np.array(avg_se).reshape(int(curr_sem_data.shape[1] / avg_per_n_steps))\n",
    "\n",
    "    return seed_mean, s_sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent_types = [\"human\", \"self_class\", \"random\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\",\n",
    "               \"dqn_training\", \"option_critic\"]\n",
    "game_types = [\n",
    "    \"contingency_game\", \"contingency_game_shuffled_1\"]\n",
    "plot_avg_distance(game_types, agent_types, avg_per_n_steps=1, level=1)\n",
    "plot_avg_distance(game_types, agent_types, avg_per_n_steps=1, level=1999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_types = [\"human\", \"self_class\", \"random\", \"a2c_training\", \"trpo_training\", \"acer_training\", \"ppo2_training\",\n",
    "               \"dqn_training\", \"keep_close\", \"option_critic\"]\n",
    "game_types = [\"change_agent_game\"]\n",
    "plot_avg_distance(game_types, agent_types, avg_per_n_steps=1, level=1)\n",
    "plot_avg_distance(game_types, agent_types, avg_per_n_steps=1, level=1999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e3518b5e748bda8c6ad8a882adecef27032776b0873e78d4c8acdb02d48a971"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
